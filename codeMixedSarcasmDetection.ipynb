{"cells":[{"cell_type":"code","source":["!python --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dMJvR3YqtHm","executionInfo":{"status":"ok","timestamp":1746267757668,"user_tz":-330,"elapsed":159,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"}},"outputId":"ae061e39-6a5a-47e7-a347-7ecee158b82a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.11.12\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10527,"status":"ok","timestamp":1746267694433,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"},"user_tz":-330},"id":"f3yWZD3nnKxo","outputId":"dd98c1b2-9747-4382-f2ed-cd1b74d06b7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.4.26)\n","Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"]}],"source":["!pip install vaderSentiment"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ACW0Q1iv8441","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747236271819,"user_tz":-330,"elapsed":25211,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"}},"outputId":"c86a22a3-5689-43cb-db73-da1d8ccffd04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LG7RKabx0kqJ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import string\n","import re\n","from ast import literal_eval\n","import nltk\n","from nltk.corpus import stopwords\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePV2s35Hb-mX"},"outputs":[],"source":["import torch\n","from transformers import AutoModel, AutoTokenizer\n","from sentence_transformers import SentenceTransformer\n","\n","# Check if GPU is available\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# ft_model = fasttext.load_model(\"cc.en.300.bin\")\n"]},{"cell_type":"code","source":["def to_lower(text):\n","    \"\"\"Converts text to lowercase.\"\"\"\n","    return text.lower()"],"metadata":{"id":"FokU0AzyZVA-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kHGuIjqR0qkR"},"outputs":[],"source":["exclude = string.punctuation\n","def removePunctuation(text):\n","    return text.translate(str.maketrans('','',exclude))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":780,"status":"ok","timestamp":1741853982754,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"},"user_tz":-330},"id":"PU7R4fwW1Qgr","outputId":"e64fdac0-4aec-4fd3-9ed2-1df0c1384c8a"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["\n","\n","import urllib.request\n","url = 'https://github.com/shad-datascience/ML_Projects/blob/main/stop_hinglish.txt'\n","file_Path = 'hinglish_text'\n","urllib.request.urlretrieve(url, file_Path)\n","\n","\n","# Download NLTK stop words (if not already downloaded)\n","nltk.download(\"stopwords\")\n","\n","# Load Hinglish stop words from file\n","def load_stop_words(file_path):\n","    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","        stop_words = set(word.strip().lower() for word in file.readlines())  # Normalize words\n","    return stop_words\n","\n","# Load Hinglish stop words\n","# hinglish_stop_words = load_stop_words(\"/content/drive/MyDrive/Dataset/stop_hinglish.txt\")\n","\n","hinglish_stop_words = load_stop_words(\"hinglish_text\")\n","\n","# Load English stop words from NLTK\n","english_stop_words = set(stopwords.words(\"english\"))\n","\n","# Combine both stop words lists\n","all_stop_words = hinglish_stop_words.union(english_stop_words)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0j_BPst09pv"},"outputs":[],"source":["\n","# Function to remove stop words (Hinglish + English)\n","def remove_stop_words(text):\n","    if isinstance(text, str):  # Ensure input is a string\n","        words = text.split()\n","        filtered_words = [word for word in words if word.lower() not in all_stop_words]\n","        return \" \".join(filtered_words)\n","    return text  # Return original if not a string (handles NaN values)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EhwcofTg108J"},"outputs":[],"source":["slang_dict = {\n","    \"lmao\": \"laughing my ass off\",\n","    \"rofl\": \"rolling on the floor laughing\",\n","    \"afaik\": \"as far as I know\",\n","    \"bcoz\": \"because\",\n","    \"frnd\": \"friend\",\n","    \"yaar\": \"friend\",\n","    \"mast\": \"awesome\",\n","    \"jhakaas\": \"superb\",\n","    \"sahi\": \"great\",\n","    \"bhai\": \"brother\",\n","    \"bro\": \"brother\",\n","    \"dost\": \"friend\",\n","    \"paka\": \"sure\",\n","    \"nai\": \"no\",\n","    \"koi nahi\": \"no one\",\n","    \"jldi\": \"jaldi\",\n","    \"aalsi\": \"lazy\",\n","    \"pakka\": \"sure\",\n","    \"biryani\": \"amazing\",\n","    \"scene hai\": \"there is a situation\",\n","    \"tight\": \"intoxicated\",\n","    \"lag gaye\": \"we are in trouble\",\n","    \"fix hai\": \"it is certain\",\n","    \"chill maar\": \"relax\",\n","    \"rapchik\": \"cool\",\n","    \"fadu\": \"amazing\",\n","    \"senti\": \"emotional\",\n","    \"jhakkas\": \"amazing\",\n","    \"kadak\": \"strong\",\n","    \"bindaas\": \"carefree\",\n","    \"haanikarak\": \"dangerous\",\n","    \"kaand\": \"big trouble\",\n","    \"faltu\": \"useless\",\n","    \"bhasad\": \"mess\",\n","    \"mamu\": \"dude\",\n","    \"tera kya scene hai?\": \"what's your plan?\",\n","    \"lafda\": \"problem\",\n","    \"locha\": \"issue\",\n","    \"jumla\": \"false promise\",\n","    \"khopdi tod\": \"mind-blowing\",\n","    \"chep\": \"clingy person\",\n","    \"lukkha\": \"useless guy\",\n","    \"matlab\": \"meaning\",\n","    \"chalu\": \"smart\",\n","    \"bawaal\": \"chaotic\",\n","    \"att\": \"attitude\",\n","    \"op\": \"overpowered\",\n","    \"hatt\": \"move away\",\n","    \"sahi hai\": \"it's good\",\n","    \"lit\": \"amazing\",\n","    \"supari\": \"contract killing\",\n","    \"ragra\": \"beaten badly\",\n","    \"maal\": \"attractive person\",\n","    \"item\": \"hot girl\",\n","    \"pataka\": \"attractive girl\",\n","    \"set hai\": \"everything is fine\",\n","    \"chindi\": \"cheap\",\n","    \"beedu\": \"close friend\",\n","    \"kat gaya\": \"got tricked\",\n","    \"tatti\": \"bad\",\n","    \"bakwaas\": \"nonsense\",\n","    \"scene on hai\": \"things are happening\",\n","    \"scene off hai\": \"not happening\",\n","    \"fix hai\": \"certain\",\n","    \"trip maar\": \"enjoy\",\n","    \"chhapri\": \"wannabe\",\n","    \"bhaiya\": \"elder brother\",\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYuO5-Jw19R-"},"outputs":[],"source":["def expand_slang(text):\n","    words = text.split()\n","    expanded_words = [slang_dict.get(word.lower(), word) for word in words]  # Replace slang\n","    return \" \".join(expanded_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6XyL7Z32NcC"},"outputs":[],"source":["def preprocessing(text):\n","    lower = to_lower(text)\n","    rem_punct = removePunctuation(lower)\n","    rem_stop = remove_stop_words(rem_punct)\n","    text = expand_slang(rem_stop)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i22B0Cypb4GO"},"outputs":[],"source":["xlmr_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n","xlmr_model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n","\n","def get_xlmr_embedding(text):\n","    tokens = xlmr_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n","    with torch.no_grad():\n","        output = xlmr_model(**tokens)\n","    return output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yh87oig2mmcl"},"outputs":[],"source":["#sentiment pipeline\n","sentiment_analyzer = VS()\n","\n","def sentiments(tweet):\n","    sentiment = sentiment_analyzer.polarity_scores(tweet)\n","    features = [sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound']]\n","    #features = pandas.DataFrame(features)\n","    return features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldPyqPztk8R2"},"outputs":[],"source":["def expand_ndarray_series(series):\n","    \"\"\"Expands a pandas Series containing ndarray values into a DataFrame with separate columns.\"\"\"\n","    array_data = np.vstack(series.values)\n","    expanded_columns = [f\"feature_{i}\" for i in range(array_data.shape[1])]\n","    return pd.DataFrame(array_data, columns=expanded_columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LcNtq-MUokzk"},"outputs":[],"source":["import re\n","import nltk\n","import pandas as pd\n","import numpy as np\n","from transformers import BertTokenizer\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline\n","\n","\n","class PreprocessingTransformer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        X = pd.Series(X)\n","        return X.apply(preprocessing)\n","\n","class XLMREmbeddingTransformer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return X.apply(get_xlmr_embedding)\n","\n","class ExpandNDArrayTransformer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return expand_ndarray_series(X)\n","\n","pipeline1 = Pipeline([\n","    ('preprocessing', PreprocessingTransformer()),\n","    ('embedding', XLMREmbeddingTransformer()),\n","    ('expand', ExpandNDArrayTransformer())\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQOyUrgqqKys"},"outputs":[],"source":["import re\n","import nltk\n","import pandas as pd\n","import numpy as np\n","from transformers import BertTokenizer\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline\n","\n","\n","class PreprocessingTransformer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        X = pd.Series(X)\n","        return X.apply(preprocessing)\n","\n","class SentimentTransformer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return X.apply(sentiments)\n","\n","class ExpandNDArrayTransformer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return expand_ndarray_series(X)\n","\n","pipeline2 = Pipeline([\n","    ('preprocessing', PreprocessingTransformer()),\n","    ('sentiments',SentimentTransformer()),\n","    ('expand', ExpandNDArrayTransformer())\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1741848556466,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"},"user_tz":-330},"id":"DPk7FQRorTfV","outputId":"dc076b64-27a3-4848-e9a7-7103f0b753ad"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 1, ..., 1, 1, 1])"]},"metadata":{},"execution_count":39}],"source":["train = pd.read_csv(\"/content/drive/MyDrive/Hackathon/train.csv\")\n","\n","y_train = train['Label']\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","le = le.fit(['YES','NO'])\n","y_train = le.transform(y_train)\n","y_train\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1741848556584,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"},"user_tz":-330},"id":"gdAEHzLAFZOD","outputId":"1c4ce2a8-3462-4f46-cacb-8fb9dc4bfcd5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, ..., 1, 1, 0])"]},"metadata":{},"execution_count":40}],"source":["valid = pd.read_csv(\"/content/drive/MyDrive/Hackathon/test.csv\")\n","\n","y_valid = valid['Label']\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","le = le.fit(['YES','NO'])\n","y_valid = le.transform(y_valid)\n","y_valid\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BRsD1BClLfcr"},"outputs":[],"source":["from sklearn.pipeline import Pipeline,FeatureUnion\n","\n","# Concatenation of both pipelines\n","class DataFrameConcatenator(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return pd.concat(X, axis=1)\n","\n","# Defining the feature processing pipeline\n","feature_pipeline = FeatureUnion([\n","    (\"pipeline1\", pipeline1),\n","    (\"pipeline2\", pipeline2)\n","])\n","\n","# from sklearn.neural_network import MLPClassifier\n","\n","# mlp_sklearn = MLPClassifier(\n","#     hidden_layer_sizes=(512, 256, 128, 64),  # Equivalent to your Keras hidden layers\n","#     activation='relu',   # ReLU activation in all hidden layers\n","#     solver='adam',       # Adam optimizer\n","#     alpha=0.0001,        # L2 regularization (to help generalization, as dropout is missing)\n","#     learning_rate='adaptive',  # Adjusts learning rate dynamically\n","#     max_iter=500,        # Number of iterations (epochs)\n","#     random_state=42\n","# )\n"]},{"cell_type":"code","source":["import cloudpickle\n","# Save the model\n","with open(\"/content/drive/MyDrive/Hackathon/Hackarena/feature_pipeline.pkl\", \"wb\") as f:\n","    cloudpickle.dump(feature_pipeline, f)"],"metadata":{"id":"iU3tSuPMPotg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139515,"status":"ok","timestamp":1741848824870,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"},"user_tz":-330},"id":"IPw_qpws2zXl","outputId":"9296035e-1378-4a6c-abb2-f5dd2320dd40"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 42ms/step - accuracy: 0.8752 - loss: 0.3451 - val_accuracy: 0.9625 - val_loss: 0.2008\n","Epoch 2/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9588 - loss: 0.1188 - val_accuracy: 0.9559 - val_loss: 0.0996\n","Epoch 3/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9628 - loss: 0.0966 - val_accuracy: 0.9782 - val_loss: 0.0623\n","Epoch 4/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9686 - loss: 0.0773 - val_accuracy: 0.9597 - val_loss: 0.1319\n","Epoch 5/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9692 - loss: 0.0762 - val_accuracy: 0.9777 - val_loss: 0.0506\n","Epoch 6/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9697 - loss: 0.0785 - val_accuracy: 0.9787 - val_loss: 0.0523\n","Epoch 7/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9741 - loss: 0.0619 - val_accuracy: 0.9545 - val_loss: 0.0916\n","Epoch 8/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9693 - loss: 0.0644 - val_accuracy: 0.9791 - val_loss: 0.0526\n","Epoch 9/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9706 - loss: 0.0689 - val_accuracy: 0.9782 - val_loss: 0.0527\n","Epoch 10/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9730 - loss: 0.0641 - val_accuracy: 0.9810 - val_loss: 0.0473\n","Epoch 11/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9759 - loss: 0.0541 - val_accuracy: 0.9749 - val_loss: 0.0579\n","Epoch 12/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9749 - loss: 0.0593 - val_accuracy: 0.9787 - val_loss: 0.0480\n","Epoch 13/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9794 - loss: 0.0540 - val_accuracy: 0.9734 - val_loss: 0.0554\n","Epoch 14/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9786 - loss: 0.0515 - val_accuracy: 0.9815 - val_loss: 0.0485\n","Epoch 15/100\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9783 - loss: 0.0522 - val_accuracy: 0.9782 - val_loss: 0.0499\n","Epoch 15: early stopping\n","Restoring model weights from the end of the best epoch: 10.\n","Test Accuracy: 0.9810\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","X_train = feature_pipeline.transform(train['Tweet'])\n","X_valid = feature_pipeline.transform(valid['Tweet'])\n","\n","# Define the MLP model\n","def create_mlp():\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),  # First hidden layer\n","        tf.keras.layers.BatchNormalization(),\n","        layers.Dropout(0.4),\n","\n","        layers.Dense(256, activation='relu'),  # Second hidden layer\n","        tf.keras.layers.BatchNormalization(),\n","        layers.Dropout(0.4),\n","\n","        layers.Dense(128, activation='relu'),  # Third hidden layer\n","        tf.keras.layers.BatchNormalization(),\n","        layers.Dropout(0.4),\n","\n","        layers.Dense(64, activation='relu'),  # fourth hidden layer\n","        tf.keras.layers.BatchNormalization(),\n","        layers.Dropout(0.4),\n","\n","        layers.Dense(1, activation='sigmoid')  # Output layer (binary classification)\n","    ])\n","\n","    # Compile the model\n","    model.compile(optimizer='adam',\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","    return model\n","\n","# Create model\n","mlp_model = create_mlp()\n","\n","# Define Early Stopping\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',  # Monitor validation loss\n","    patience=5,  # Stop training if no improvement for 5 epochs\n","    restore_best_weights=True,  # Restore the best weights when stopping\n","    verbose=1\n",")\n","\n","\n","# Train the model with Early Stopping\n","mlp_model.fit(\n","    X_train, y_train,\n","    epochs=100,\n","    batch_size=64,\n","    validation_data=(X_valid, y_valid),\n","    callbacks=[early_stopping],  # Apply Early Stopping\n","    verbose=1\n",")\n","\n","# Evaluate the model\n","test_loss, test_acc = mlp_model.evaluate(X_valid, y_valid, verbose=0)\n","print(f\"Test Accuracy: {test_acc:.4f}\")\n"]},{"cell_type":"code","source":["mlp_model.save('/content/drive/MyDrive/Hackathon/Hackarena/mlp_model.keras')"],"metadata":{"id":"sILPZOzrSRHa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install vaderSentiment"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LEgGNjNcVBTe","executionInfo":{"status":"ok","timestamp":1741849646215,"user_tz":-330,"elapsed":5890,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"}},"outputId":"117ab827-6cc3-4a3d-a6bb-510fc5b0efc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.1.31)\n","Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import string\n","import re\n","from ast import literal_eval\n","import nltk\n","from nltk.corpus import stopwords\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n","import re\n","import nltk\n","import pandas as pd\n","import numpy as np\n","from transformers import BertTokenizer\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n"],"metadata":{"id":"-Bzk2GnYVALM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow import keras\n","import torch"],"metadata":{"id":"-oyQEsQAVXyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install vaderSentiment"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x1cTjsnuQFuz","executionInfo":{"status":"ok","timestamp":1741881906148,"user_tz":-330,"elapsed":5275,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"}},"outputId":"339c2939-0941-4778-e1e7-aa55e27052f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.1.31)\n","Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"]}]},{"cell_type":"code","source":["!pip install VaderSentiment"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nAWrymo0vkTB","executionInfo":{"status":"ok","timestamp":1747236327519,"user_tz":-330,"elapsed":3652,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"}},"outputId":"0caaab33-634f-426b-af29-48cc71d39beb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting VaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from VaderSentiment) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->VaderSentiment) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->VaderSentiment) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->VaderSentiment) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->VaderSentiment) (2025.4.26)\n","Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: VaderSentiment\n","Successfully installed VaderSentiment-3.3.2\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WX9RxJqOZThw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"mpI3iaSsF41d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747236554929,"user_tz":-330,"elapsed":15789,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"}},"outputId":"ed51ff76-418f-410b-bb48-4128db091bb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0.9944623]], dtype=float32)"]},"metadata":{},"execution_count":6}],"source":["import joblib\n","from tensorflow import keras\n","\n","model = keras.models.load_model(\"/content/drive/MyDrive/Hackathon/Hackarena/mlp_model.keras\")\n","pipeline = joblib.load(\"/content/drive/MyDrive/Hackathon/Hackarena/feature_pipeline.pkl\")\n","\n","text_input= \"beta tum to bade heavy driver ho!!\"\n","processed_input = pipeline.transform(text_input)\n","\n","model.predict(processed_input)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"yEZgQvdVTS59","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1741882052038,"user_tz":-330,"elapsed":22,"user":{"displayName":"Shad Jamil","userId":"01082820483775185312"}},"outputId":"00a6af4e-972c-40d0-fcdd-0babe4e15cca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.4.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":[],"metadata":{"id":"Su3TPEwAQaUD"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}